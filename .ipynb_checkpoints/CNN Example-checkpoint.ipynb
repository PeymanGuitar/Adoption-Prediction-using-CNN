{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network is a special type of Artificial Neural Network where you use convolutional tricks to\n",
    "add convolutional layers to process and classify images. CNN is a great deep learning module in computer vision to classify\n",
    "images and videos. Our goal is come up with a convolutional model to predict the adoptation time that a pet is \n",
    "adopted. In both the training set and the test set we have the following categories:\n",
    "    \n",
    "1 - Pet was adopted between 1 and 7 days (1st week) after being listed. \n",
    "\n",
    "\n",
    "2 - Pet was adopted between 8 and 30 days (1st month) after being listed. \n",
    "\n",
    "\n",
    "3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed. \n",
    "\n",
    "\n",
    "4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 - Buidling the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Keras packages that we need to do the project.\n",
    "\n",
    "Keras the main library in the computer vision.\n",
    "\n",
    "1. Sequential is used to initialize the CNN.\n",
    "\n",
    "\n",
    "2. Convolution2D is for the convolution step. Note that images are two dimensional.\n",
    "\n",
    "\n",
    "3. MaxPooling2D takes care of the maxpooling step.\n",
    "\n",
    "\n",
    "4. Flatten will change the results gotten from maxpooling into verctors.\n",
    "\n",
    "\n",
    "5. Dense is used to get the fully connected layers in the Aritificial Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets recall that CNN is consisted of 4 steps.\n",
    "\n",
    "\n",
    "1. Convolution\n",
    "\n",
    "2. Max Pooling\n",
    "\n",
    "3. Flattening\n",
    "\n",
    "4. Full Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following command, Convolution2D(32,(3,3)) 32 is the number of feature detectors. 3 the number of rows in each filter, 3 is the number of columns, input images are conveted into 3D arrays as they are color pictures.\n",
    "\n",
    "We want our activation function to be rectified linear unit since we want non-liearity as pixels are not linearly related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Convolution2D(32,(3,3), input_shape = (64,64,3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pooling step, we slide a 2 * 2 matrix over the Feature detectors obtained from the first step with a stride of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crucial step as we take the matrices of the previous step and covert it to a large vector to be used in the ANN.\n",
    "\n",
    "We won't lose information as we have taken the crucial information from the images in the feature detector step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Full Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make a traditional ANN to classify the images by using the package Dense. We still need to use the \n",
    "activation function to be rectified linear unit to add non-linearity to our model. Based on our experience it is better\n",
    "to be taken as a power of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer has to be softmax since we have only 4 categories. (If it was binary then we needed to choose sigmoid). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 4, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compile our model. Our optimizer is the adam agorithm, our loss function is categorical_crossentropy as \n",
    "we have 4 categories. The matric is of course accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 - Fitting the CNN to images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to process images to avoid overfitting. We want to avoid good result only on our training set and not good results on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 760 images belonging to 4 classes.\n",
      "Found 324 images belonging to 4 classes.\n",
      "Epoch 1/25\n",
      "760/760 [==============================] - 314s 413ms/step - loss: 1.1405 - acc: 0.5054 - val_loss: 2.1497 - val_acc: 0.2592\n",
      "Epoch 2/25\n",
      "760/760 [==============================] - 304s 400ms/step - loss: 0.5536 - acc: 0.7969 - val_loss: 3.2613 - val_acc: 0.2414\n",
      "Epoch 3/25\n",
      "760/760 [==============================] - 316s 416ms/step - loss: 0.2222 - acc: 0.9309 - val_loss: 4.6320 - val_acc: 0.2288\n",
      "Epoch 4/25\n",
      "760/760 [==============================] - 303s 399ms/step - loss: 0.1040 - acc: 0.9702 - val_loss: 4.9504 - val_acc: 0.2687\n",
      "Epoch 5/25\n",
      "760/760 [==============================] - 307s 404ms/step - loss: 0.0726 - acc: 0.9801 - val_loss: 5.2099 - val_acc: 0.2654\n",
      "Epoch 6/25\n",
      "760/760 [==============================] - 315s 414ms/step - loss: 0.0522 - acc: 0.9851 - val_loss: 5.7777 - val_acc: 0.2622\n",
      "Epoch 7/25\n",
      "760/760 [==============================] - 295s 388ms/step - loss: 0.0373 - acc: 0.9901 - val_loss: 6.6513 - val_acc: 0.1947\n",
      "Epoch 8/25\n",
      "760/760 [==============================] - 295s 389ms/step - loss: 0.0305 - acc: 0.9917 - val_loss: 6.5660 - val_acc: 0.2256\n",
      "Epoch 9/25\n",
      "760/760 [==============================] - 290s 382ms/step - loss: 0.0319 - acc: 0.9908 - val_loss: 7.1834 - val_acc: 0.2097\n",
      "Epoch 10/25\n",
      "760/760 [==============================] - 306s 402ms/step - loss: 0.0399 - acc: 0.9882 - val_loss: 7.1812 - val_acc: 0.2188\n",
      "Epoch 11/25\n",
      "760/760 [==============================] - 315s 415ms/step - loss: 0.0171 - acc: 0.9952 - val_loss: 6.7383 - val_acc: 0.2624\n",
      "Epoch 12/25\n",
      "760/760 [==============================] - 302s 397ms/step - loss: 0.0274 - acc: 0.9925 - val_loss: 7.1742 - val_acc: 0.2474\n",
      "Epoch 13/25\n",
      "760/760 [==============================] - 292s 384ms/step - loss: 0.0265 - acc: 0.9927 - val_loss: 7.1187 - val_acc: 0.2340\n",
      "Epoch 14/25\n",
      "760/760 [==============================] - 286s 377ms/step - loss: 0.0168 - acc: 0.9955 - val_loss: 7.2161 - val_acc: 0.2740\n",
      "Epoch 15/25\n",
      "760/760 [==============================] - 286s 377ms/step - loss: 0.0079 - acc: 0.9979 - val_loss: 7.2567 - val_acc: 0.2566\n",
      "Epoch 16/25\n",
      "760/760 [==============================] - 414s 544ms/step - loss: 0.0263 - acc: 0.9921 - val_loss: 7.9380 - val_acc: 0.2568\n",
      "Epoch 17/25\n",
      "760/760 [==============================] - 284s 373ms/step - loss: 0.0157 - acc: 0.9950 - val_loss: 8.0124 - val_acc: 0.2499\n",
      "Epoch 18/25\n",
      "760/760 [==============================] - 285s 376ms/step - loss: 0.0106 - acc: 0.9972 - val_loss: 7.8522 - val_acc: 0.2630\n",
      "Epoch 19/25\n",
      "760/760 [==============================] - 287s 377ms/step - loss: 0.0124 - acc: 0.9966 - val_loss: 8.1216 - val_acc: 0.2585\n",
      "Epoch 20/25\n",
      "760/760 [==============================] - 286s 377ms/step - loss: 0.0268 - acc: 0.9928 - val_loss: 8.0133 - val_acc: 0.2409\n",
      "Epoch 21/25\n",
      "760/760 [==============================] - 287s 377ms/step - loss: 0.0096 - acc: 0.9970 - val_loss: 7.9184 - val_acc: 0.2558\n",
      "Epoch 22/25\n",
      "760/760 [==============================] - 286s 376ms/step - loss: 0.0165 - acc: 0.9952 - val_loss: 7.5807 - val_acc: 0.2622\n",
      "Epoch 23/25\n",
      "760/760 [==============================] - 286s 377ms/step - loss: 0.0142 - acc: 0.9959 - val_loss: 8.4165 - val_acc: 0.2531\n",
      "Epoch 24/25\n",
      "760/760 [==============================] - 288s 378ms/step - loss: 0.0112 - acc: 0.9970 - val_loss: 7.8833 - val_acc: 0.2253\n",
      "Epoch 25/25\n",
      "760/760 [==============================] - 291s 383ms/step - loss: 0.0072 - acc: 0.9975 - val_loss: 8.6242 - val_acc: 0.2168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120529490>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('datasets/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('datasets/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 760,\n",
    "                         epochs = 25,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 324)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
